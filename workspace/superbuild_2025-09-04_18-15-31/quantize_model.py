import tensorflow as tf\nfrom pathlib import Path\n\ndef quantize_model(model_path, quantized_model_path):\n    """Quantize a saved model for reduced model size and faster inference.\n\n    Args:\n        model_path: Path to the saved model directory.\n        quantized_model_path: Path to save the quantized model.\n    """\n    # Load the saved model\n    model = tf.keras.models.load_model(model_path)\n\n    # Create a quantization configuration\n    quantize_config = tf.keras.optimizers.experimental.Optimizer.get_quantization_config()\n\n    # Quantize the model\n    quantized_model = tf.keras.models.clone_model(\n        model,\n        clone_function=lambda layer: tf.keras.layers.Activation('linear')(layer),\n        input_tensors=None,\n        optimizer_config=quantize_config\n    )\n\n    # Compile the quantized model\n    quantized_model.compile(\n        optimizer='adam',\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n        metrics=['accuracy']\n    )\n\n    # Save the quantized model\n    quantized_model.save(quantized_model_path)\n\n    print(f'Original model size: {get_model_size(model_path):.2f} MB')\n    print(f'Quantized model size: {get_model_size(quantized_model_path):.2f} MB')\n\ndef get_model_size(model_path):\n    """Calculate the size of a saved model in megabytes.\n\n    Args:\n        model_path: Path to the saved model directory.\n\n    Returns:\n        The model size in megabytes.\n    """\n    model_size = sum(f.stat().st_size for f in Path(model_path).rglob('*.*'))\n    return model_size / (1024 * 1024)\n\nif __name__ == '__main__':\n    model_path = 'path/to/original/model'\n    quantized_model_path = 'path/to/quantized/model'\n    quantize_model(model_path, quantized_model_path)\n