import tensorflow as tf\n\ndef quantize_model(model_path, quantized_model_path):\n    """Quantize a saved model to int8 precision for deployment.\n    \n    Args:\n        model_path: Path to the saved model directory.\n        quantized_model_path: Path to save the quantized model.\n    """\n    # Load the saved model\n    model = tf.keras.models.load_model(model_path)\n    \n    # Create a quantization config\n    quantize_config = tf.keras.optimizers.experimental.Optimizer.get_quantization_config()\n    quantize_config.experimental_relax_float32_conv_params = True\n\n    # Quantize the model\n    quantized_model = tf.quantization.quantize_model(model, quantize_config=quantize_config)\n\n    # Save the quantized model\n    quantized_model.save(quantized_model_path)\n\n    print(f'Quantized model saved to: {quantized_model_path}')\n\nif __name__ == '__main__':\n    model_path = 'path/to/saved/model'\n    quantized_model_path = 'path/to/quantized/model'\n    quantize_model(model_path, quantized_model_path)\n