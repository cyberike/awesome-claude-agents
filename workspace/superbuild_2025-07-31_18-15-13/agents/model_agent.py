import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass ModelAgent:\n    def __init__(self):\n        self.model_name = "EleutherAI/gpt-neo-1.3B"\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name)\n        self.model = self.model.to('cpu')\n        self.model = torch.quantization.quantize_dynamic(self.model, {torch.nn.Linear}, dtype=torch.qint8)\n\n    def generate_response(self, prompt):\n        input_ids = self.tokenizer.encode(prompt, return_tensors="pt")\n        output = self.model.generate(input_ids, max_length=100, num_return_sequences=1)\n        response = self.tokenizer.decode(output[0], skip_special_tokens=True)\n        return response