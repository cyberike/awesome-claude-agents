import tensorflow as tfdef quantize_model(model_path, quantized_model_path):    # Load the saved model    model = tf.keras.models.load_model(model_path)        # Convert the model to TensorFlow Lite format    converter = tf.lite.TFLiteConverter.from_keras_model(model)    converter.optimizations = [tf.lite.Optimize.DEFAULT]        # Perform post-training quantization    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]    converter.inference_input_type = tf.int8    converter.inference_output_type = tf.int8        # Convert the model    quantized_tflite_model = converter.convert()        # Save the quantized model    with open(quantized_model_path, 'wb') as f:        f.write(quantized_tflite_model)    print(f'Quantized model saved at: {quantized_model_path}')# Example usagemodel_path = 'path/to/your/model.h5'quantized_model_path = 'path/to/save/quantized_model.tflite'quantize_model(model_path, quantized_model_path)