import tensorflow as tfdef quantize_model(model_path, quantized_model_path):    # Load the saved model    model = tf.keras.models.load_model(model_path)        # Convert the model to TensorFlow Lite format    converter = tf.lite.TFLiteConverter.from_keras_model(model)    converter.optimizations = [tf.lite.Optimize.DEFAULT]        # Perform post-training quantization    converter.target_spec.supported_types = [tf.float16]    quantized_model = converter.convert()        # Save the quantized model    with open(quantized_model_path, 'wb') as f:        f.write(quantized_model)        print(f'Model quantized and saved at: {quantized_model_path}')# Example usagemodel_path = 'path/to/your/model.h5'quantized_model_path = 'path/to/save/quantized_model.tflite'quantize_model(model_path, quantized_model_path)